{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":22881,"databundleVersionId":1552852,"sourceType":"competition"},{"sourceId":7415831,"sourceType":"datasetVersion","datasetId":4314036}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\n# Enable memory growth for the single GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    print(\"Running on GPU:\", gpus)\nelse:\n    print(\"No GPU found, using CPU\")\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.applications import MobileNetV2,MobileNetV3Small,EfficientNetV2B0\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\nimport tensorflow.keras.backend as K\nimport datetime\nfrom sklearn.metrics import roc_curve, auc\nimport cv2\nimport random\nfrom tensorflow.keras.mixed_precision import set_global_policy, Policy\n\n\n\n# Initialize MirroredStrategy for single GPU\nstrategy = tf.distribute.MirroredStrategy()  # Auto-detects available GPUs\nprint(\"Number of GPUs in strategy:\", strategy.num_replicas_in_sync)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:44:16.117456Z","iopub.execute_input":"2025-07-29T01:44:16.117694Z","iopub.status.idle":"2025-07-29T01:44:30.066735Z","shell.execute_reply.started":"2025-07-29T01:44:16.117670Z","shell.execute_reply":"2025-07-29T01:44:30.065944Z"}},"outputs":[{"name":"stderr","text":"2025-07-29 01:44:17.590134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753753457.765297      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753753457.828274      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Running on GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nNumber of GPUs in strategy: 1\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753753470.031693      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_SIZE = (112,112)\nSPOOF_IMG_SIZE = (64, 64)\nVERIFY_IMG_SIZE = (112,112)\nDATA_DIR = \"/kaggle/input/11-785-fall-20-homework-2-part-2\"\nTRAIN_DIR = f\"{DATA_DIR}/classification_data/train_data\"\nVERIFICATION_FILE = os.path.join(DATA_DIR, \"verification_pairs_val.txt\")\nCASIA_DIR = \"/kaggle/input/casia-fasd/casia-fasd\"  # Path to CASIA-FASD dataset\nNUM_PAIRS_PER_PERSON = 18\nEMBEDDING_DIM = 128\nLEARNING_RATE = 0.0001\nVAL_SPLIT = 0.2\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:44:30.068036Z","iopub.execute_input":"2025-07-29T01:44:30.068439Z","iopub.status.idle":"2025-07-29T01:44:30.072736Z","shell.execute_reply.started":"2025-07-29T01:44:30.068420Z","shell.execute_reply":"2025-07-29T01:44:30.072034Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#   Face verification Preprocessing","metadata":{}},{"cell_type":"code","source":"\ndef load_and_preprocess_image(image_path, img_size=IMG_SIZE):\n    if isinstance(image_path, str):\n        image = cv2.imread(image_path)\n        if image is None:\n            raise ValueError(f\"Could not load image: {image_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    else:\n        image = image_path\n    image = cv2.resize(image, img_size)\n    image = image.astype(np.float32)\n    image = np.clip(image, 0, 255)\n    return image\n\ndef create_pairs_from_verification_file(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = []\n    with open(verification_file_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) >= 3:\n                img1_path = os.path.join(base_dir, parts[0])\n                img2_path = os.path.join(base_dir, parts[1])\n                label = int(parts[2])\n                pairs.append((img1_path, img2_path, label))\n    return pairs\n\ndef create_pairs_from_classification_data(classification_dir=TRAIN_DIR, num_pairs_per_person=NUM_PAIRS_PER_PERSON):\n    pairs = []\n    person_dirs = [d for d in os.listdir(classification_dir) \n                  if os.path.isdir(os.path.join(classification_dir, d))]\n    \n    for person_dir in person_dirs:\n        person_path = os.path.join(classification_dir, person_dir)\n        images = [f for f in os.listdir(person_path) \n                 if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if len(images) >= 2:\n            for _ in range(num_pairs_per_person):\n                img1, img2 = random.sample(images, 2)\n                img1_path = os.path.join(person_path, img1)\n                img2_path = os.path.join(person_path, img2)\n                pairs.append((img1_path, img2_path, 1))\n    \n    num_negative_pairs = len(pairs)\n    for _ in range(num_negative_pairs):\n        person1, person2 = random.sample(person_dirs, 2)\n        person1_path = os.path.join(classification_dir, person1)\n        person2_path = os.path.join(classification_dir, person2)\n        images1 = [f for f in os.listdir(person1_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        images2 = [f for f in os.listdir(person2_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if images1 and images2:\n            img1 = random.choice(images1)\n            img2 = random.choice(images2)\n            img1_path = os.path.join(person1_path, img1)\n            img2_path = os.path.join(person2_path, img2)\n            pairs.append((img1_path, img2_path, 0))\n    \n    random.shuffle(pairs)\n    return pairs\n\ndef create_data_augmentation_layer():\n    return keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n    ])\n\ndef prepare_dataset(pairs, batch_size=BATCH_SIZE, shuffle=True, augment=True):\n    def load_pair(img1_path, img2_path, label):\n        img1 = load_and_preprocess_image(img1_path.numpy().decode(\"utf-8\"))\n        img2 = load_and_preprocess_image(img2_path.numpy().decode(\"utf-8\"))\n        \n        # Convert to tensors with proper shape\n        img1 = tf.convert_to_tensor(img1, dtype=tf.float32)\n        img2 = tf.convert_to_tensor(img2, dtype=tf.float32)\n        \n        return (img1, img2, label)\n    \n    img1_paths = [pair[0] for pair in pairs]\n    img2_paths = [pair[1] for pair in pairs]\n    labels = [pair[2] for pair in pairs]\n    \n    dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels))\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=len(pairs))\n    \n    dataset = dataset.map(\n        lambda p1, p2, l: tf.py_function(\n            load_pair, [p1, p2, l], [tf.float32, tf.float32, tf.int32]\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    dataset = dataset.map(\n        lambda img1, img2, label: (\n            (tf.ensure_shape(img1, [*IMG_SIZE, 3]),\n             tf.ensure_shape(img2, [*IMG_SIZE, 3])),\n            tf.ensure_shape(label, [])\n        )\n    )\n    \n    dataset = dataset.batch(batch_size)\n    \n    # Apply augmentation AFTER batching\n    if augment:\n        augment_layer = create_data_augmentation_layer()\n        def augment_batch(batch_images, batch_labels):\n            img1_batch, img2_batch = batch_images\n            img1_batch = augment_layer(img1_batch)\n            img2_batch = augment_layer(img2_batch)\n            return (img1_batch, img2_batch), batch_labels\n        \n        dataset = dataset.map(augment_batch, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef load_verification_data(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = create_pairs_from_verification_file(verification_file_path, base_dir)\n    images1, images2, labels = [], [], []\n    \n    for img1_path, img2_path, label in pairs:\n        try:\n            img1 = load_and_preprocess_image(img1_path)\n            img2 = load_and_preprocess_image(img2_path)\n            images1.append(img1)\n            images2.append(img2)\n            labels.append(label)\n        except Exception as e:\n            print(f\"Error loading pair {img1_path}, {img2_path}: {e}\")\n            continue\n    \n    return np.array(images1), np.array(images2), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:44:30.073417Z","iopub.execute_input":"2025-07-29T01:44:30.073692Z","iopub.status.idle":"2025-07-29T01:44:30.133805Z","shell.execute_reply.started":"2025-07-29T01:44:30.073665Z","shell.execute_reply":"2025-07-29T01:44:30.133172Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Anti-spoofing preprocessing","metadata":{}},{"cell_type":"code","source":"def create_antispoofing_generators(data_dir, img_size=IMG_SIZE, batch_size=BATCH_SIZE, val_split=VAL_SPLIT):\n    \"\"\"Create data generators for anti-spoofing using CASIA-FASD train and test folders.\"\"\"\n    train_datagen = ImageDataGenerator(\n        rotation_range=10,\n        horizontal_flip=True,\n        zoom_range=0.2,\n        shear_range=0.2,\n        # brightness_range=[0.8, 1.2],\n        validation_split=val_split\n    )\n    \n    val_datagen = ImageDataGenerator(\n        validation_split=val_split\n    )\n    \n    test_datagen = ImageDataGenerator()\n    \n    train_generator = train_datagen.flow_from_directory(\n        os.path.join(data_dir, 'train'),\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='binary',\n\n        shuffle=True,\n        subset='training'\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        os.path.join(data_dir, 'train'),\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='binary',\n\n        shuffle=False,\n        subset='validation'\n    )\n    \n    test_generator = test_datagen.flow_from_directory(\n        os.path.join(data_dir, 'test'),\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='binary',\n\n        shuffle=False\n    )\n    \n    return train_generator, val_generator, test_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:44:30.134522Z","iopub.execute_input":"2025-07-29T01:44:30.135020Z","iopub.status.idle":"2025-07-29T01:44:30.152366Z","shell.execute_reply.started":"2025-07-29T01:44:30.134982Z","shell.execute_reply":"2025-07-29T01:44:30.151688Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Combine Dataset ","metadata":{}},{"cell_type":"code","source":"print(\"Preparing face verification datasets...\")\ntrain_pairs = create_pairs_from_classification_data()\ntrain_verify_dataset = prepare_dataset(train_pairs, batch_size=BATCH_SIZE, augment=True)\nval_pairs = create_pairs_from_verification_file()\nval_verify_dataset = prepare_dataset(val_pairs, batch_size=BATCH_SIZE, augment=False)\n\nprint(\"Creating anti-spoofing data generators...\")\nspoof_train_generator, spoof_val_generator, spoof_test_generator = create_antispoofing_generators(CASIA_DIR)\n\ndef combined_generator():\n    verify_iter = iter(train_verify_dataset)\n    spoof_iter = iter(spoof_train_generator)\n    while True:\n        try:\n            (img1, img2), verify_label = next(verify_iter)\n            spoof_img, spoof_label = next(spoof_iter)\n            batch_size = min(img1.shape[0], spoof_img.shape[0])\n            img1 = img1[:batch_size]\n            img2 = img2[:batch_size]\n            verify_label = tf.cast(verify_label[:batch_size], tf.float32)  # Cast to float32\n            spoof_img = spoof_img[:batch_size]\n            spoof_label = spoof_label[:batch_size]\n            yield (img1, img2, spoof_img), {'verify': verify_label, 'spoof': spoof_label}\n        except StopIteration:\n            break\n\ndef val_generator():\n    verify_iter = iter(val_verify_dataset)\n    spoof_iter = iter(spoof_val_generator)\n    while True:\n        try:\n            (img1, img2), verify_label = next(verify_iter)\n            spoof_img, spoof_label = next(spoof_iter)\n            batch_size = min(img1.shape[0], spoof_img.shape[0])\n            img1 = img1[:batch_size]\n            img2 = img2[:batch_size]\n            verify_label = tf.cast(verify_label[:batch_size], tf.float32)  # Cast to float32\n            spoof_img = spoof_img[:batch_size]\n            spoof_label = spoof_label[:batch_size]\n            yield (img1, img2, spoof_img), {'verify': verify_label, 'spoof': spoof_label}\n        except StopIteration:\n            break\n\n# Create datasets\ntrain_dataset = tf.data.Dataset.from_generator(\n    combined_generator,\n    output_types=((tf.float32, tf.float32, tf.float32), {'verify': tf.float32, 'spoof': tf.float32}),\n    output_shapes=(([None, *IMG_SIZE, 3], [None, *IMG_SIZE, 3], [None, *IMG_SIZE, 3]), {'verify': [None], 'spoof': [None]})\n).prefetch(tf.data.AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_generator(\n    val_generator,\n    output_types=((tf.float32, tf.float32, tf.float32), {'verify': tf.float32, 'spoof': tf.float32}),\n    output_shapes=(([None, *IMG_SIZE, 3], [None, *IMG_SIZE, 3], [None, *IMG_SIZE, 3]), {'verify': [None], 'spoof': [None]})\n).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:44:30.153919Z","iopub.execute_input":"2025-07-29T01:44:30.154616Z","iopub.status.idle":"2025-07-29T01:48:46.407376Z","shell.execute_reply.started":"2025-07-29T01:44:30.154595Z","shell.execute_reply":"2025-07-29T01:48:46.406814Z"}},"outputs":[{"name":"stdout","text":"Preparing face verification datasets...\nCreating anti-spoofing data generators...\nFound 46198 images belonging to 2 classes.\nFound 11549 images belonging to 2 classes.\nFound 65786 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def create_embedding_network(embedding_dim=EMBEDDING_DIM):\n    \"\"\"Create embedding network for shared feature extraction.\"\"\"\n    base_model = EfficientNetV2B0(\n        weights=\"imagenet\",\n        include_top=False,\n        input_shape=(*IMG_SIZE, 3)\n    )\n    base_model.trainable = True\n    \n    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n    x = keras.applications.efficientnet_v2.preprocess_input(inputs)\n    x = base_model(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    x = layers.Dense(512, name=\"embedding_dense1\")(x)\n    x = layers.BatchNormalization(name=\"embedding_bn1\")(x)\n    x = layers.Activation('relu', name=\"activation_bn1\")(x)\n    embeddings = layers.Dropout(0.4, name=\"embedding_dropout1\")(x)\n\n    \n    \n         \n    return keras.Model(inputs,embeddings, name = \"embedding_network\")\n\ndef create_combined_model(verify_image_size = IMG_SIZE, spoof_image_size = IMG_SIZE, embedding_dim=EMBEDDING_DIM, learning_rate=LEARNING_RATE):\n    \"\"\"Create multi-task Siamese model for verification and anti-spoofing.\"\"\"\n    with strategy.scope():\n        input1 = keras.Input(shape=(*verify_image_size, 3), name=\"image1\")\n        input2 = keras.Input(shape=(*verify_image_size, 3), name=\"image2\")\n        spoof_input = keras.Input(shape=(*spoof_image_size, 3), name=\"spoof_input\")\n        \n        embedding_network = create_embedding_network(embedding_dim)\n        \n        embedding1 = embedding_network(input1)\n        embedding2 = embedding_network(input2)\n        spoof_embedding= embedding_network(spoof_input)\n        \n\n        concat = Concatenate(name=\"similarity_dense1\")([embedding1, embedding2])\n        dense = Dense(embedding_dim, activation = 'relu', name ='concatenation_layer')(concat)\n        drop_out = layers.Dropout(0.3, name=\"similarity_dropout\")(dense)\n        verify_output = layers.Dense(1, activation='sigmoid', name=\"verify\")(drop_out)\n        \n        \n        \n        spoof_output = layers.Dense(1, activation='sigmoid', name=\"spoof\")(spoof_embedding)\n        \n        model = keras.Model(\n            inputs=[input1, input2, spoof_input],\n            outputs=[verify_output, spoof_output]\n        )\n        \n        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n        model.compile(\n            optimizer=optimizer,\n            loss={\n                'verify': 'binary_crossentropy',\n                'spoof': 'binary_crossentropy'\n            },\n            loss_weights={\n                'verify': 0.6,\n                'spoof': 0.4\n            },\n            metrics={\n                'verify': ['accuracy'],\n                'spoof': ['accuracy']\n            }\n        )\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:48:46.408064Z","iopub.execute_input":"2025-07-29T01:48:46.408305Z","iopub.status.idle":"2025-07-29T01:48:46.416364Z","shell.execute_reply.started":"2025-07-29T01:48:46.408280Z","shell.execute_reply":"2025-07-29T01:48:46.415740Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, train_dataset, val_dataset, epochs=100):\n    \"\"\"Train multi-task model with verification and anti-spoofing datasets.\"\"\"\n    checkpoint = callbacks.ModelCheckpoint(\n        f\"best_anti_spoofing_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.keras\",\n        monitor=\"val_verify_accuracy\",\n        save_best_only=True,\n        mode=\"auto\",\n        verbose=1\n    )\n    early_stopping = callbacks.EarlyStopping(\n        monitor=\"val_verify_accuracy\",\n        patience=5,\n        restore_best_weights=True,\n        verbose=1,\n        mode = 'max'\n    )\n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor=\"val_verify_accuracy\",\n        factor=0.1,\n        patience=2,\n        min_lr=1e-6,\n        verbose=1\n    )\n    \n    \n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=10,\n        verbose=1,\n        callbacks=[checkpoint, early_stopping, reduce_lr]\n    )\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:48:46.416943Z","iopub.execute_input":"2025-07-29T01:48:46.417161Z","iopub.status.idle":"2025-07-29T01:48:46.434107Z","shell.execute_reply.started":"2025-07-29T01:48:46.417146Z","shell.execute_reply":"2025-07-29T01:48:46.433448Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"Building combined model...\")\nmodel = create_combined_model()\nmodel.summary()\nprint(\"Training model...\")\nhistory = train_model(model, train_dataset, val_dataset)\npd.DataFrame(history.history).to_csv('model_metrics.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T01:48:46.434803Z","iopub.execute_input":"2025-07-29T01:48:46.435062Z","iopub.status.idle":"2025-07-29T05:59:01.915275Z","shell.execute_reply.started":"2025-07-29T01:48:46.435040Z","shell.execute_reply":"2025-07-29T05:59:01.914447Z"}},"outputs":[{"name":"stdout","text":"Building combined model...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_network   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m6,577,232\u001b[0m │ image1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ image2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ spoof_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dense1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ embedding_networ… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ embedding_networ… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenation_layer │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m262,400\u001b[0m │ similarity_dense… │\n│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spoof_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenation_la… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ verify (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ similarity_dropo… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spoof (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m513\u001b[0m │ embedding_networ… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_network   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,577,232</span> │ image1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ image2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ spoof_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dense1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_networ… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ embedding_networ… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenation_layer │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ similarity_dense… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spoof_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenation_la… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ verify (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ similarity_dropo… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spoof (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ embedding_networ… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,840,402\u001b[0m (26.09 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,840,402</span> (26.09 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,778,770\u001b[0m (25.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,778,770</span> (25.86 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m61,632\u001b[0m (240.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,632</span> (240.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Training model...\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1753753784.814037      36 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1_1/embedding_network_1/efficientnetv2-b0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1753753796.973334      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"   4500/Unknown \u001b[1m1556s\u001b[0m 332ms/step - loss: 0.3880 - spoof_accuracy: 0.9738 - spoof_loss: 0.0609 - verify_accuracy: 0.5378 - verify_loss: 0.7151","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_verify_accuracy improved from -inf to 0.71880, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1633s\u001b[0m 349ms/step - loss: 0.3880 - spoof_accuracy: 0.9738 - spoof_loss: 0.0609 - verify_accuracy: 0.5378 - verify_loss: 0.7151 - val_loss: 0.2900 - val_spoof_accuracy: 0.9906 - val_spoof_loss: 0.0511 - val_verify_accuracy: 0.7188 - val_verify_loss: 0.5278 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - loss: 0.2544 - spoof_accuracy: 0.9999 - spoof_loss: 0.0012 - verify_accuracy: 0.7489 - verify_loss: 0.5076\nEpoch 2: val_verify_accuracy improved from 0.71880 to 0.75983, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1458s\u001b[0m 324ms/step - loss: 0.2544 - spoof_accuracy: 0.9999 - spoof_loss: 0.0012 - verify_accuracy: 0.7489 - verify_loss: 0.5076 - val_loss: 0.2864 - val_spoof_accuracy: 0.9721 - val_spoof_loss: 0.0817 - val_verify_accuracy: 0.7598 - val_verify_loss: 0.4884 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 0.2237 - spoof_accuracy: 0.9998 - spoof_loss: 7.3306e-04 - verify_accuracy: 0.7912 - verify_loss: 0.4467\nEpoch 3: val_verify_accuracy improved from 0.75983 to 0.77255, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1464s\u001b[0m 325ms/step - loss: 0.2237 - spoof_accuracy: 0.9998 - spoof_loss: 7.3305e-04 - verify_accuracy: 0.7912 - verify_loss: 0.4467 - val_loss: 0.2694 - val_spoof_accuracy: 0.9681 - val_spoof_loss: 0.0623 - val_verify_accuracy: 0.7726 - val_verify_loss: 0.4738 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 0.2044 - spoof_accuracy: 0.9999 - spoof_loss: 5.5495e-04 - verify_accuracy: 0.8156 - verify_loss: 0.4082\nEpoch 4: val_verify_accuracy improved from 0.77255 to 0.78755, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1454s\u001b[0m 323ms/step - loss: 0.2044 - spoof_accuracy: 0.9999 - spoof_loss: 5.5492e-04 - verify_accuracy: 0.8156 - verify_loss: 0.4082 - val_loss: 0.2557 - val_spoof_accuracy: 0.9724 - val_spoof_loss: 0.0659 - val_verify_accuracy: 0.7875 - val_verify_loss: 0.4441 - learning_rate: 1.0000e-04\nEpoch 5/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 0.1933 - spoof_accuracy: 1.0000 - spoof_loss: 1.7406e-04 - verify_accuracy: 0.8285 - verify_loss: 0.3864\nEpoch 5: val_verify_accuracy did not improve from 0.78755\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1490s\u001b[0m 331ms/step - loss: 0.1933 - spoof_accuracy: 1.0000 - spoof_loss: 1.7406e-04 - verify_accuracy: 0.8285 - verify_loss: 0.3864 - val_loss: 0.3087 - val_spoof_accuracy: 0.9284 - val_spoof_loss: 0.1518 - val_verify_accuracy: 0.7807 - val_verify_loss: 0.4626 - learning_rate: 1.0000e-04\nEpoch 6/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - loss: 0.1841 - spoof_accuracy: 1.0000 - spoof_loss: 1.8501e-04 - verify_accuracy: 0.8388 - verify_loss: 0.3679\nEpoch 6: val_verify_accuracy improved from 0.78755 to 0.80209, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1528s\u001b[0m 339ms/step - loss: 0.1841 - spoof_accuracy: 1.0000 - spoof_loss: 1.8500e-04 - verify_accuracy: 0.8388 - verify_loss: 0.3679 - val_loss: 0.2305 - val_spoof_accuracy: 0.9992 - val_spoof_loss: 0.0077 - val_verify_accuracy: 0.8021 - val_verify_loss: 0.4507 - learning_rate: 1.0000e-04\nEpoch 7/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 0.1759 - spoof_accuracy: 0.9999 - spoof_loss: 2.4933e-04 - verify_accuracy: 0.8473 - verify_loss: 0.3516\nEpoch 7: val_verify_accuracy did not improve from 0.80209\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1497s\u001b[0m 332ms/step - loss: 0.1759 - spoof_accuracy: 0.9999 - spoof_loss: 2.4933e-04 - verify_accuracy: 0.8473 - verify_loss: 0.3516 - val_loss: 0.2537 - val_spoof_accuracy: 0.9844 - val_spoof_loss: 0.0390 - val_verify_accuracy: 0.7904 - val_verify_loss: 0.4684 - learning_rate: 1.0000e-04\nEpoch 8/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 0.1682 - spoof_accuracy: 1.0000 - spoof_loss: 1.0834e-04 - verify_accuracy: 0.8552 - verify_loss: 0.3362\nEpoch 8: val_verify_accuracy did not improve from 0.80209\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1480s\u001b[0m 329ms/step - loss: 0.1682 - spoof_accuracy: 1.0000 - spoof_loss: 1.0834e-04 - verify_accuracy: 0.8552 - verify_loss: 0.3362 - val_loss: 0.3160 - val_spoof_accuracy: 0.9335 - val_spoof_loss: 0.1623 - val_verify_accuracy: 0.8010 - val_verify_loss: 0.4659 - learning_rate: 1.0000e-04\nEpoch 9/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 0.1565 - spoof_accuracy: 1.0000 - spoof_loss: 8.2770e-05 - verify_accuracy: 0.8681 - verify_loss: 0.3128\nEpoch 9: val_verify_accuracy improved from 0.80209 to 0.81034, saving model to best_anti_spoofing_model_20250729_014849.keras\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1492s\u001b[0m 331ms/step - loss: 0.1565 - spoof_accuracy: 1.0000 - spoof_loss: 8.2766e-05 - verify_accuracy: 0.8681 - verify_loss: 0.3128 - val_loss: 0.2447 - val_spoof_accuracy: 0.9836 - val_spoof_loss: 0.0409 - val_verify_accuracy: 0.8103 - val_verify_loss: 0.4477 - learning_rate: 1.0000e-05\nEpoch 10/10\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - loss: 0.1503 - spoof_accuracy: 1.0000 - spoof_loss: 9.3522e-05 - verify_accuracy: 0.8743 - verify_loss: 0.3005\nEpoch 10: val_verify_accuracy did not improve from 0.81034\n\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1507s\u001b[0m 335ms/step - loss: 0.1503 - spoof_accuracy: 1.0000 - spoof_loss: 9.3517e-05 - verify_accuracy: 0.8743 - verify_loss: 0.3005 - val_loss: 0.2385 - val_spoof_accuracy: 0.9942 - val_spoof_loss: 0.0201 - val_verify_accuracy: 0.8103 - val_verify_loss: 0.4549 - learning_rate: 1.0000e-05\nRestoring model weights from the end of the best epoch: 9.\n","output_type":"stream"}],"execution_count":8}]}