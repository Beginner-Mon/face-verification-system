{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":22881,"databundleVersionId":1552852,"sourceType":"competition"},{"sourceId":478382,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":384131,"modelId":403518}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.applications import MobileNetV2,MobileNetV3Small,EfficientNetV2B0\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\nimport datetime\nfrom sklearn.metrics import roc_curve, auc\nimport cv2\nimport random\nfrom tensorflow.keras.mixed_precision import set_global_policy, Policy\n\n# # Enable memory growth for both T4 GPUs\n# gpus = tf.config.list_physical_devices('GPU')\n# print(\"Available GPUs:\", [gpu.name for gpu in gpus])  # Should list two T4 GPUs\n# for gpu in gpus:\n#     tf.config.experimental.set_memory_growth(gpu, True)\n\n\n\n# Initialize MirroredStrategy\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\nprint(\"Number of GPUs in strategy:\", strategy.num_replicas_in_sync)  # Should print 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:30:08.265920Z","iopub.execute_input":"2025-07-19T12:30:08.266145Z","iopub.status.idle":"2025-07-19T12:30:23.434003Z","shell.execute_reply.started":"2025-07-19T12:30:08.266120Z","shell.execute_reply":"2025-07-19T12:30:23.433065Z"}},"outputs":[{"name":"stderr","text":"2025-07-19 12:30:10.085019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752928210.293160      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752928210.357691      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Number of GPUs in strategy: 2\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752928223.396780      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"BATCH_SIZE = 64\nIMG_SIZE = (112, 112)\nDATA_DIR = \"/kaggle/input/11-785-fall-20-homework-2-part-2\"\nTRAIN_DIR = f\"{DATA_DIR}/classification_data/train_data\"\nVERIFICATION_FILE = os.path.join(DATA_DIR, \"verification_pairs_val.txt\")\nNUM_PAIRS_PER_PERSON = 25\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:30:23.435226Z","iopub.execute_input":"2025-07-19T12:30:23.436218Z","iopub.status.idle":"2025-07-19T12:30:23.441522Z","shell.execute_reply.started":"2025-07-19T12:30:23.436185Z","shell.execute_reply":"2025-07-19T12:30:23.440805Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndef load_and_preprocess_image(image_path, img_size=IMG_SIZE):\n    if isinstance(image_path, str):\n        image = cv2.imread(image_path)\n        if image is None:\n            raise ValueError(f\"Could not load image: {image_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    else:\n        image = image_path\n    image = cv2.resize(image, img_size)\n    image = image.astype(np.float32)\n    image = np.clip(image, 0, 255)\n    return image\n\ndef create_pairs_from_verification_file(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = []\n    with open(verification_file_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) >= 3:\n                img1_path = os.path.join(base_dir, parts[0])\n                img2_path = os.path.join(base_dir, parts[1])\n                label = int(parts[2])\n                pairs.append((img1_path, img2_path, label))\n    return pairs\n\ndef create_pairs_from_classification_data(classification_dir=TRAIN_DIR, num_pairs_per_person=NUM_PAIRS_PER_PERSON):\n    pairs = []\n    person_dirs = [d for d in os.listdir(classification_dir) \n                  if os.path.isdir(os.path.join(classification_dir, d))]\n    \n    for person_dir in person_dirs:\n        person_path = os.path.join(classification_dir, person_dir)\n        images = [f for f in os.listdir(person_path) \n                 if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if len(images) >= 2:\n            for _ in range(num_pairs_per_person):\n                img1, img2 = random.sample(images, 2)\n                img1_path = os.path.join(person_path, img1)\n                img2_path = os.path.join(person_path, img2)\n                pairs.append((img1_path, img2_path, 1))\n    \n    num_negative_pairs = len(pairs)\n    for _ in range(num_negative_pairs):\n        person1, person2 = random.sample(person_dirs, 2)\n        person1_path = os.path.join(classification_dir, person1)\n        person2_path = os.path.join(classification_dir, person2)\n        images1 = [f for f in os.listdir(person1_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        images2 = [f for f in os.listdir(person2_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if images1 and images2:\n            img1 = random.choice(images1)\n            img2 = random.choice(images2)\n            img1_path = os.path.join(person1_path, img1)\n            img2_path = os.path.join(person2_path, img2)\n            pairs.append((img1_path, img2_path, 0))\n    \n    random.shuffle(pairs)\n    return pairs\n\ndef create_data_augmentation_layer():\n    return keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n    ])\n\ndef prepare_dataset(pairs, batch_size=BATCH_SIZE, shuffle=True, augment=True):\n    def load_pair(img1_path, img2_path, label):\n        img1 = load_and_preprocess_image(img1_path.numpy().decode(\"utf-8\"))\n        img2 = load_and_preprocess_image(img2_path.numpy().decode(\"utf-8\"))\n        \n        # Convert to tensors with proper shape\n        img1 = tf.convert_to_tensor(img1, dtype=tf.float32)\n        img2 = tf.convert_to_tensor(img2, dtype=tf.float32)\n        \n        return (img1, img2, label)\n    \n    img1_paths = [pair[0] for pair in pairs]\n    img2_paths = [pair[1] for pair in pairs]\n    labels = [pair[2] for pair in pairs]\n    \n    dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels))\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=len(pairs))\n    \n    dataset = dataset.map(\n        lambda p1, p2, l: tf.py_function(\n            load_pair, [p1, p2, l], [tf.float32, tf.float32, tf.int32]\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    dataset = dataset.map(\n        lambda img1, img2, label: (\n            (tf.ensure_shape(img1, [*IMG_SIZE, 3]),\n             tf.ensure_shape(img2, [*IMG_SIZE, 3])),\n            tf.ensure_shape(label, [])\n        )\n    )\n    \n    dataset = dataset.batch(batch_size)\n    \n    # Apply augmentation AFTER batching\n    if augment:\n        augment_layer = create_data_augmentation_layer()\n        def augment_batch(batch_images, batch_labels):\n            img1_batch, img2_batch = batch_images\n            img1_batch = augment_layer(img1_batch)\n            img2_batch = augment_layer(img2_batch)\n            return (img1_batch, img2_batch), batch_labels\n        \n        dataset = dataset.map(augment_batch, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef load_verification_data(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = create_pairs_from_verification_file(verification_file_path, base_dir)\n    images1, images2, labels = [], [], []\n    \n    for img1_path, img2_path, label in pairs:\n        try:\n            img1 = load_and_preprocess_image(img1_path)\n            img2 = load_and_preprocess_image(img2_path)\n            images1.append(img1)\n            images2.append(img2)\n            labels.append(label)\n        except Exception as e:\n            print(f\"Error loading pair {img1_path}, {img2_path}: {e}\")\n            continue\n    \n    return np.array(images1), np.array(images2), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:30:23.443472Z","iopub.execute_input":"2025-07-19T12:30:23.444126Z","iopub.status.idle":"2025-07-19T12:30:23.509885Z","shell.execute_reply.started":"2025-07-19T12:30:23.444096Z","shell.execute_reply":"2025-07-19T12:30:23.509148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Prepare training dataset (from classification folder)\ntrain_pairs = create_pairs_from_classification_data()\ntrain_dataset = prepare_dataset(train_pairs, batch_size=BATCH_SIZE, augment=True)\n\n# Prepare validation dataset (from verification file)\nval_images1, val_images2, val_labels = load_verification_data()\nval_pairs = create_pairs_from_verification_file()\nval_dataset = prepare_dataset(val_pairs, batch_size=BATCH_SIZE, augment=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:30:23.510717Z","iopub.execute_input":"2025-07-19T12:30:23.510981Z","iopub.status.idle":"2025-07-19T12:34:34.606166Z","shell.execute_reply.started":"2025-07-19T12:30:23.510962Z","shell.execute_reply":"2025-07-19T12:34:34.605598Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# def contrastive_loss(y_true, y_pred, margin=MARGIN):\n#     y_true = tf.cast(y_true, tf.float32)\n#     y_pred = tf.cast(y_pred, tf.float32)\n#     loss = y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0))\n#     return tf.reduce_mean(loss)\n\n# def euclidean_distance(embeddings):\n#     embedding1, embedding2 = embeddings\n#     return tf.sqrt(tf.reduce_sum(tf.square(embedding1 - embedding2), axis=1, keepdims=True))\n\n# def cosine_distance(embeddings):\n#     embedding1, embedding2 = embeddings\n#     cosine_sim = tf.reduce_sum(embedding1 * embedding2, axis=1, keepdims=True)\n#     return 1 - cosine_sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:34:34.606915Z","iopub.execute_input":"2025-07-19T12:34:34.607127Z","iopub.status.idle":"2025-07-19T12:34:34.610652Z","shell.execute_reply.started":"2025-07-19T12:34:34.607110Z","shell.execute_reply":"2025-07-19T12:34:34.610026Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"EMBEDDING_DIM = 128\nFREEZE_BACKBONE = True\nLEARNING_RATE = 0.0001\ndef create_embedding_network(embedding_dim=EMBEDDING_DIM, freeze_backbone=FREEZE_BACKBONE):\n    \n    base_model = EfficientNetV2B0(\n        weights=\"imagenet\", include_top=False, input_shape=(*IMG_SIZE, 3)\n    )\n    # if freeze_backbone:\n    #     base_model.trainable = False\n    base_model.trainable = True\n        \n\n    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n    x = keras.applications.efficientnet_v2.preprocess_input(inputs)\n    \n    x = base_model(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    x = layers.Dense(512, name=\"embedding_dense1\")(x)\n    x = layers.BatchNormalization(name=\"embedding_bn1\")(x)\n    x = layers.Activation('relu', name = \"activation_bn1\")(x)\n    x = layers.Dropout(0.4, name=\"embedding_dropout1\")(x)\n    \n    # x = layers.Dense(256, name=\"embedding_dense2\")(x)\n    # x = layers.BatchNormalization(name=\"embedding_bn2\")(x)\n    # x = layers.Activation('relu', name = \"activation_bn2\")(x)\n    # x = layers.Dropout(0.3, name=\"embedding_dropout2\")(x)\n    \n\n\n    return keras.Model(inputs, x)\n\ndef unfreeze_backbone(model):\n    backbone = model.layers[2]  # Base model layer\n    backbone.trainable = True\n    return model\n\n\n\ndef create_siamese_model(input_shape=(*IMG_SIZE, 3), embedding_dim=EMBEDDING_DIM, \n                      \n                        freeze_backbone=FREEZE_BACKBONE, learning_rate=LEARNING_RATE):\n    input1 = keras.Input(shape=input_shape, name=\"image1\")\n    input2 = keras.Input(shape=input_shape, name=\"image2\")\n    \n    embedding_network = create_embedding_network(embedding_dim, freeze_backbone)\n    \n    embedding1 = embedding_network(input1)\n    embedding2 = embedding_network(input2)\n    \n    concat = Concatenate()([embedding1, embedding2])\n    dense = Dense(512, activation = 'relu')(concat)\n    output = Dense(1, activation = 'sigmoid')(dense)\n    \n    model = keras.Model(inputs=[input1, input2], outputs=output)\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate if freeze_backbone else learning_rate/10)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T12:34:34.611302Z","iopub.execute_input":"2025-07-19T12:34:34.611475Z","iopub.status.idle":"2025-07-19T12:34:34.633294Z","shell.execute_reply.started":"2025-07-19T12:34:34.611461Z","shell.execute_reply":"2025-07-19T12:34:34.632731Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# Build model\n# model = create_siamese_model()\n# model.summary()\nmodel = load_model(\"/kaggle/working/best_Siamese_model_20250719_123439.keras\")\nmodel.summary()\n\noptimizer = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])\n# Define callbacks\n\ncheckpoint = callbacks.ModelCheckpoint(\n    f\"best_Siamese_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.keras\"\n    , monitor=\"val_accuracy\", save_best_only=True, mode=\"auto\", verbose=1\n)\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_accuracy\", patience=3, restore_best_weights=True, verbose=1\n)\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_accuracy\", factor=0.2, patience=2, min_lr=1e-6, verbose=1\n)\n# Train model (Phase 1: Frozen backbone)\nhistory = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=100,\n        verbose=1,\n        callbacks=[checkpoint, early_stopping, reduce_lr],\n        initial_epoch = 20\n    \n    )\n\n# Save final model\nmodel.save(f\"final_Siamese_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.keras\")\n\n# Save training history\npd.DataFrame(history.history).to_csv(f\"history_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:09:00.959657Z","iopub.execute_input":"2025-07-19T15:09:00.960223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m6,577,232\u001b[0m │ image1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ image2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ functional_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ functional_6[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m524,800\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m513\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,577,232</span> │ image1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ image2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ functional_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,184,373\u001b[0m (80.81 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,184,373</span> (80.81 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,040,913\u001b[0m (26.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,040,913</span> (26.86 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m61,632\u001b[0m (240.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,632</span> (240.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m14,081,828\u001b[0m (53.72 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,081,828</span> (53.72 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 21/100\n\u001b[1m2039/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4:53\u001b[0m 270ms/step - accuracy: 0.9052 - loss: 0.2329","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Optional: Fine-tune (Phase 2)\n# model = unfreeze_backbone(model)\n# model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='binary_crossentropy')\n# model.fit(\n#         train_dataset,\n#         validation_data=val_dataset,\n#         epochs=100,\n#         initial_epoch = 10,\n#         verbose=1,\n#         callbacks=[checkpoint, early_stopping, reduce_lr],\n    \n#     )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:04:56.292081Z","iopub.status.idle":"2025-07-19T15:04:56.292328Z","shell.execute_reply.started":"2025-07-19T15:04:56.292212Z","shell.execute_reply":"2025-07-19T15:04:56.292224Z"}},"outputs":[],"execution_count":null}]}