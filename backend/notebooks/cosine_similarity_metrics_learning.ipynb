{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":22881,"databundleVersionId":1552852,"sourceType":"competition"},{"sourceId":478382,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":384131,"modelId":403518}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.applications import MobileNetV2,MobileNetV3Small,EfficientNetV2B0\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\nimport tensorflow.keras.backend as K\nimport datetime\nfrom sklearn.metrics import roc_curve, auc\nimport cv2\nimport random\nfrom tensorflow.keras.mixed_precision import set_global_policy, Policy\n\n# # Enable memory growth for both T4 GPUs\n# gpus = tf.config.list_physical_devices('GPU')\n# print(\"Available GPUs:\", [gpu.name for gpu in gpus])  # Should list two T4 GPUs\n# for gpu in gpus:\n#     tf.config.experimental.set_memory_growth(gpu, True)\n\n\n\n# Initialize MirroredStrategy\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\nprint(\"Number of GPUs in strategy:\", strategy.num_replicas_in_sync)  # Should print 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:04:12.624830Z","iopub.execute_input":"2025-07-22T10:04:12.625348Z","iopub.status.idle":"2025-07-22T10:04:12.636689Z","shell.execute_reply.started":"2025-07-22T10:04:12.625319Z","shell.execute_reply":"2025-07-22T10:04:12.635849Z"}},"outputs":[{"name":"stdout","text":"Number of GPUs in strategy: 2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"BATCH_SIZE = 64\nIMG_SIZE = (112, 112)\nDATA_DIR = \"/kaggle/input/11-785-fall-20-homework-2-part-2\"\nTRAIN_DIR = f\"{DATA_DIR}/classification_data/train_data\"\nVERIFICATION_FILE = os.path.join(DATA_DIR, \"verification_pairs_val.txt\")\nNUM_PAIRS_PER_PERSON = 25\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:04:12.638353Z","iopub.execute_input":"2025-07-22T10:04:12.638585Z","iopub.status.idle":"2025-07-22T10:04:12.659390Z","shell.execute_reply.started":"2025-07-22T10:04:12.638559Z","shell.execute_reply":"2025-07-22T10:04:12.658478Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\ndef load_and_preprocess_image(image_path, img_size=IMG_SIZE):\n    if isinstance(image_path, str):\n        image = cv2.imread(image_path)\n        if image is None:\n            raise ValueError(f\"Could not load image: {image_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    else:\n        image = image_path\n    image = cv2.resize(image, img_size)\n    image = image.astype(np.float32)\n    image = np.clip(image, 0, 255)\n    return image\n\ndef create_pairs_from_verification_file(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = []\n    with open(verification_file_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) >= 3:\n                img1_path = os.path.join(base_dir, parts[0])\n                img2_path = os.path.join(base_dir, parts[1])\n                label = int(parts[2])\n                pairs.append((img1_path, img2_path, label))\n    return pairs\n\ndef create_pairs_from_classification_data(classification_dir=TRAIN_DIR, num_pairs_per_person=NUM_PAIRS_PER_PERSON):\n    pairs = []\n    person_dirs = [d for d in os.listdir(classification_dir) \n                  if os.path.isdir(os.path.join(classification_dir, d))]\n    \n    for person_dir in person_dirs:\n        person_path = os.path.join(classification_dir, person_dir)\n        images = [f for f in os.listdir(person_path) \n                 if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if len(images) >= 2:\n            for _ in range(num_pairs_per_person):\n                img1, img2 = random.sample(images, 2)\n                img1_path = os.path.join(person_path, img1)\n                img2_path = os.path.join(person_path, img2)\n                pairs.append((img1_path, img2_path, 1))\n    \n    num_negative_pairs = len(pairs)\n    for _ in range(num_negative_pairs):\n        person1, person2 = random.sample(person_dirs, 2)\n        person1_path = os.path.join(classification_dir, person1)\n        person2_path = os.path.join(classification_dir, person2)\n        images1 = [f for f in os.listdir(person1_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        images2 = [f for f in os.listdir(person2_path) \n                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        if images1 and images2:\n            img1 = random.choice(images1)\n            img2 = random.choice(images2)\n            img1_path = os.path.join(person1_path, img1)\n            img2_path = os.path.join(person2_path, img2)\n            pairs.append((img1_path, img2_path, 0))\n    \n    random.shuffle(pairs)\n    return pairs\n\ndef create_data_augmentation_layer():\n    return keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n    ])\n\ndef prepare_dataset(pairs, batch_size=BATCH_SIZE, shuffle=True, augment=True):\n    def load_pair(img1_path, img2_path, label):\n        img1 = load_and_preprocess_image(img1_path.numpy().decode(\"utf-8\"))\n        img2 = load_and_preprocess_image(img2_path.numpy().decode(\"utf-8\"))\n        \n        # Convert to tensors with proper shape\n        img1 = tf.convert_to_tensor(img1, dtype=tf.float32)\n        img2 = tf.convert_to_tensor(img2, dtype=tf.float32)\n        \n        return (img1, img2, label)\n    \n    img1_paths = [pair[0] for pair in pairs]\n    img2_paths = [pair[1] for pair in pairs]\n    labels = [pair[2] for pair in pairs]\n    \n    dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels))\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=len(pairs))\n    \n    dataset = dataset.map(\n        lambda p1, p2, l: tf.py_function(\n            load_pair, [p1, p2, l], [tf.float32, tf.float32, tf.int32]\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    dataset = dataset.map(\n        lambda img1, img2, label: (\n            (tf.ensure_shape(img1, [*IMG_SIZE, 3]),\n             tf.ensure_shape(img2, [*IMG_SIZE, 3])),\n            tf.ensure_shape(label, [])\n        )\n    )\n    \n    dataset = dataset.batch(batch_size)\n    \n    # Apply augmentation AFTER batching\n    if augment:\n        augment_layer = create_data_augmentation_layer()\n        def augment_batch(batch_images, batch_labels):\n            img1_batch, img2_batch = batch_images\n            img1_batch = augment_layer(img1_batch)\n            img2_batch = augment_layer(img2_batch)\n            return (img1_batch, img2_batch), batch_labels\n        \n        dataset = dataset.map(augment_batch, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef load_verification_data(verification_file_path=VERIFICATION_FILE, base_dir=DATA_DIR):\n    pairs = create_pairs_from_verification_file(verification_file_path, base_dir)\n    images1, images2, labels = [], [], []\n    \n    for img1_path, img2_path, label in pairs:\n        try:\n            img1 = load_and_preprocess_image(img1_path)\n            img2 = load_and_preprocess_image(img2_path)\n            images1.append(img1)\n            images2.append(img2)\n            labels.append(label)\n        except Exception as e:\n            print(f\"Error loading pair {img1_path}, {img2_path}: {e}\")\n            continue\n    \n    return np.array(images1), np.array(images2), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:04:12.660384Z","iopub.execute_input":"2025-07-22T10:04:12.660957Z","iopub.status.idle":"2025-07-22T10:04:12.682550Z","shell.execute_reply.started":"2025-07-22T10:04:12.660918Z","shell.execute_reply":"2025-07-22T10:04:12.681686Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Prepare training dataset (from classification folder)\ntrain_pairs = create_pairs_from_classification_data()\ntrain_dataset = prepare_dataset(train_pairs, batch_size=BATCH_SIZE, augment=True)\n\n# Prepare validation dataset (from verification file)\nval_images1, val_images2, val_labels = load_verification_data()\nval_pairs = create_pairs_from_verification_file()\nval_dataset = prepare_dataset(val_pairs, batch_size=BATCH_SIZE, augment=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:04:12.683444Z","iopub.execute_input":"2025-07-22T10:04:12.683787Z","iopub.status.idle":"2025-07-22T10:06:49.926611Z","shell.execute_reply.started":"2025-07-22T10:04:12.683749Z","shell.execute_reply":"2025-07-22T10:06:49.926028Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# def contrastive_loss(y_true, y_pred, margin=MARGIN):\n#     y_true = tf.cast(y_true, tf.float32)\n#     y_pred = tf.cast(y_pred, tf.float32)\n#     loss = y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0))\n#     return tf.reduce_mean(loss)\n\ndef euclidean_distance(embeddings):\n    embedding1, embedding2 = embeddings\n    return tf.sqrt(tf.reduce_sum(tf.square(embedding1 - embedding2), axis=1, keepdims=True))\n\ndef cosine_distance(embeddings):\n    embedding1, embedding2 = embeddings\n    cosine_sim = tf.reduce_sum(embedding1 * embedding2, axis=1, keepdims=True)\n    return 1 - cosine_sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:06:49.928253Z","iopub.execute_input":"2025-07-22T10:06:49.928475Z","iopub.status.idle":"2025-07-22T10:06:49.933158Z","shell.execute_reply.started":"2025-07-22T10:06:49.928458Z","shell.execute_reply":"2025-07-22T10:06:49.932247Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"EMBEDDING_DIM = 128\nFREEZE_BACKBONE = True\nLEARNING_RATE = 0.0001\ndef create_embedding_network(embedding_dim=EMBEDDING_DIM, freeze_backbone=FREEZE_BACKBONE):\n    \n    base_model = EfficientNetV2B0(\n        weights=\"imagenet\", include_top=False, input_shape=(*IMG_SIZE, 3)\n    )\n    # if freeze_backbone:\n    #     base_model.trainable = False\n    base_model.trainable = True\n        \n\n    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n    x = keras.applications.efficientnet_v2.preprocess_input(inputs)\n    \n    x = base_model(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    x = layers.Dense(512, name=\"embedding_dense1\")(x)\n    x = layers.BatchNormalization(name=\"embedding_bn1\")(x)\n    x = layers.Activation('relu', name = \"activation_bn1\")(x)\n    x = layers.Dropout(0.4, name=\"embedding_dropout1\")(x)\n    \n    # x = layers.Dense(256, name=\"embedding_dense2\")(x)\n    # x = layers.BatchNormalization(name=\"embedding_bn2\")(x)\n    # x = layers.Activation('relu', name = \"activation_bn2\")(x)\n    # x = layers.Dropout(0.3, name=\"embedding_dropout2\")(x)\n    \n\n\n    # Final embedding layer with proper dimensionality\n    embeddings = layers.Dense(embedding_dim, name=\"final_embeddings\")(x)\n    # L2 normalize embeddings for better cosine similarity computation\n    embeddings = layers.Lambda(lambda x: K.l2_normalize(x, axis=1), name=\"l2_normalize\",output_shape=(embedding_dim,))(embeddings)\n    \n    return Model(inputs, embeddings)\n\ndef unfreeze_backbone(model):\n    backbone = model.layers[2]  # Base model layer\n    backbone.trainable = True\n    return model\n\n\n\ndef create_siamese_model(input_shape=(*IMG_SIZE, 3), embedding_dim=EMBEDDING_DIM, \n                      \n                        freeze_backbone=FREEZE_BACKBONE, learning_rate=LEARNING_RATE):\n    input1 = keras.Input(shape=input_shape, name=\"image1\")\n    input2 = keras.Input(shape=input_shape, name=\"image2\")\n    \n    embedding_network = create_embedding_network(embedding_dim, freeze_backbone)\n    \n    embedding1 = embedding_network(input1)\n    embedding2 = embedding_network(input2)\n    \n    def cosine_similarity(vectors):\n        x, y = vectors\n        return K.sum(x * y, axis=1, keepdims=True)\n    \n    similarity = layers.Lambda(cosine_similarity, name=\"cosine_similarity\",output_shape=(1,))([embedding1, embedding2])\n    \n    # Additional processing layers to learn optimal decision boundary\n    x = layers.Dense(64, activation='relu', name=\"similarity_dense1\")(similarity)\n    x = layers.Dropout(0.2, name=\"similarity_dropout\")(x)\n    # x = layers.Dense(32, activation='relu', name=\"similarity_dense2\")(x)\n    \n    # Final binary classification layer\n    output = layers.Dense(1, activation='sigmoid', name=\"binary_output\")(x)\n    \n    # Build and compile model\n    model = keras.Model(inputs=[input1, input2], outputs=output)\n    \n    model = keras.Model(inputs=[input1, input2], outputs=output)\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate if freeze_backbone else learning_rate/10)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:06:49.933953Z","iopub.execute_input":"2025-07-22T10:06:49.934566Z","iopub.status.idle":"2025-07-22T10:06:49.957027Z","shell.execute_reply.started":"2025-07-22T10:06:49.934539Z","shell.execute_reply":"2025-07-22T10:06:49.956119Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\n# Build model\nmodel = create_siamese_model()\nmodel.summary()\n# model = load_model(\"/kaggle/working/best_Siamese_model_20250719_123439.keras\")\n# model.summary()\n\noptimizer = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])\n# Define callbacks\n\ncheckpoint = callbacks.ModelCheckpoint(\n    f\"best_Siamese_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.keras\"\n    , monitor=\"val_accuracy\", save_best_only=True, mode=\"auto\", verbose=1\n)\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_accuracy\", patience=3, restore_best_weights=True, verbose=1\n)\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_accuracy\", factor=0.2, patience=2, min_lr=1e-6, verbose=1\n)\n# Train model (Phase 1: Frozen backbone)\nhistory = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=100,\n        verbose=1,\n        callbacks=[checkpoint, early_stopping, reduce_lr],\n        initial_epoch = 0\n    \n    )\n\n# Save final model\nmodel.save(f\"final_Siamese_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.keras\")\n\n# Save training history\npd.DataFrame(history.history).to_csv(f\"history_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:06:49.957966Z","iopub.execute_input":"2025-07-22T10:06:49.958177Z","iopub.status.idle":"2025-07-22T12:43:19.510394Z","shell.execute_reply.started":"2025-07-22T10:06:49.958162Z","shell.execute_reply":"2025-07-22T12:43:19.509625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_9\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m6,642,896\u001b[0m │ image1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ image2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cosine_similarity   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ functional_7[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dense1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ cosine_similarit… │\n│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ similarity_dense… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ binary_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ similarity_dropo… │\n│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,642,896</span> │ image1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ image2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cosine_similarity   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ functional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dense1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ cosine_similarit… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ similarity_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ similarity_dense… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ binary_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ similarity_dropo… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,643,089\u001b[0m (25.34 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,643,089</span> (25.34 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,581,457\u001b[0m (25.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,581,457</span> (25.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m61,632\u001b[0m (240.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,632</span> (240.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753178905.127565      93 service.cc:148] XLA service 0x7fe7e40020e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1753178905.128439      93 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1753178915.154561      93 cuda_dnn.cc:529] Loaded cuDNN version 90300\nE0000 00:00:1753178930.390102      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753178930.573647      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753178931.081389      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753178931.288178      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1753178967.581376      93 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.6536 - loss: 0.6155\nEpoch 1: val_accuracy improved from -inf to 0.76865, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1434s\u001b[0m 409ms/step - accuracy: 0.6536 - loss: 0.6155 - val_accuracy: 0.7687 - val_loss: 0.4848 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.8141 - loss: 0.4200\nEpoch 2: val_accuracy improved from 0.76865 to 0.79671, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m881s\u001b[0m 282ms/step - accuracy: 0.8141 - loss: 0.4200 - val_accuracy: 0.7967 - val_loss: 0.4442 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.8402 - loss: 0.3705\nEpoch 3: val_accuracy improved from 0.79671 to 0.80625, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m890s\u001b[0m 285ms/step - accuracy: 0.8402 - loss: 0.3705 - val_accuracy: 0.8062 - val_loss: 0.4316 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8551 - loss: 0.3397\nEpoch 4: val_accuracy improved from 0.80625 to 0.81533, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 276ms/step - accuracy: 0.8551 - loss: 0.3397 - val_accuracy: 0.8153 - val_loss: 0.4232 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.8669 - loss: 0.3150\nEpoch 5: val_accuracy did not improve from 0.81533\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m868s\u001b[0m 277ms/step - accuracy: 0.8669 - loss: 0.3150 - val_accuracy: 0.8097 - val_loss: 0.4353 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.8786 - loss: 0.2942\nEpoch 6: val_accuracy improved from 0.81533 to 0.82249, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 279ms/step - accuracy: 0.8786 - loss: 0.2942 - val_accuracy: 0.8225 - val_loss: 0.4220 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8858 - loss: 0.2795\nEpoch 7: val_accuracy improved from 0.82249 to 0.83021, saving model to best_Siamese_model_20250722_100651.keras\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m885s\u001b[0m 283ms/step - accuracy: 0.8858 - loss: 0.2795 - val_accuracy: 0.8302 - val_loss: 0.4135 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8919 - loss: 0.2661\nEpoch 8: val_accuracy did not improve from 0.83021\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 278ms/step - accuracy: 0.8919 - loss: 0.2661 - val_accuracy: 0.8262 - val_loss: 0.4270 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.8995 - loss: 0.2493\nEpoch 9: val_accuracy did not improve from 0.83021\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m923s\u001b[0m 295ms/step - accuracy: 0.8995 - loss: 0.2493 - val_accuracy: 0.8208 - val_loss: 0.4410 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.9111 - loss: 0.2253\nEpoch 10: val_accuracy did not improve from 0.83021\n\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m896s\u001b[0m 286ms/step - accuracy: 0.9111 - loss: 0.2253 - val_accuracy: 0.8260 - val_loss: 0.4405 - learning_rate: 2.0000e-05\nEpoch 10: early stopping\nRestoring model weights from the end of the best epoch: 7.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Optional: Fine-tune (Phase 2)\n# model = unfreeze_backbone(model)\n# model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='binary_crossentropy')\n# model.fit(\n#         train_dataset,\n#         validation_data=val_dataset,\n#         epochs=100,\n#         initial_epoch = 10,\n#         verbose=1,\n#         callbacks=[checkpoint, early_stopping, reduce_lr],\n    \n#     )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:43:19.511241Z","iopub.execute_input":"2025-07-22T12:43:19.511504Z","iopub.status.idle":"2025-07-22T12:43:19.515344Z","shell.execute_reply.started":"2025-07-22T12:43:19.511475Z","shell.execute_reply":"2025-07-22T12:43:19.514674Z"}},"outputs":[],"execution_count":22}]}